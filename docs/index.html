<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLAMA K8s Deployment - Academic Project</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #fff;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        
        /* Header */
        .header {
            background: linear-gradient(135deg, #2563eb 0%, #1d4ed8 100%);
            color: white;
            padding: 3rem 0;
            text-align: center;
        }
        
        .header h1 {
            font-size: 3rem;
            font-weight: 700;
            margin-bottom: 1rem;
        }
        
        .header p {
            font-size: 1.2rem;
            opacity: 0.9;
            max-width: 600px;
            margin: 0 auto;
        }
        
        /* Navigation */
        .nav {
            background: white;
            border-bottom: 1px solid #e2e8f0;
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        .nav-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 0;
        }
        
        .nav-links {
            display: flex;
            gap: 2rem;
        }
        
        .nav-links a {
            color: #333;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        
        .nav-links a:hover {
            color: #2563eb;
        }
        
        /* Sections */
        .section {
            padding: 4rem 0;
        }
        
        .section:nth-child(even) {
            background: #f8fafc;
        }
        
        .section h2 {
            font-size: 2.5rem;
            font-weight: 600;
            margin-bottom: 2rem;
            text-align: center;
        }
        
        .section h3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: #2563eb;
        }
        
        /* Cards */
        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }
        
        .card {
            background: white;
            border: 1px solid #e2e8f0;
            border-radius: 12px;
            padding: 2rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .card:hover {
            transform: translateY(-4px);
            box-shadow: 0 10px 25px -3px rgba(0, 0, 0, 0.1);
        }
        
        .card-icon {
            font-size: 2.5rem;
            margin-bottom: 1rem;
        }
        
        /* Code blocks */
        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            margin: 1rem 0;
        }
        
        .code-block pre {
            margin: 0;
        }
        
        /* Buttons */
        .btn {
            display: inline-block;
            padding: 0.75rem 1.5rem;
            background: #2563eb;
            color: white;
            text-decoration: none;
            border-radius: 8px;
            font-weight: 500;
            transition: background 0.3s;
        }
        
        .btn:hover {
            background: #1d4ed8;
        }
        
        /* Features */
        .feature-list {
            list-style: none;
            padding: 0;
        }
        
        .feature-list li {
            padding: 0.5rem 0;
            border-bottom: 1px solid #e2e8f0;
        }
        
        .feature-list li:before {
            content: "‚úì";
            color: #10b981;
            font-weight: bold;
            margin-right: 0.5rem;
        }
        
        /* Footer */
        .footer {
            background: #1e293b;
            color: white;
            text-align: center;
            padding: 2rem 0;
            margin-top: 4rem;
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            .header h1 {
                font-size: 2rem;
            }
            
            .nav-links {
                display: none;
            }
            
            .card-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="container">
            <h1>LLAMA K8s Deployment</h1>
            <p>A comprehensive Kubernetes-based deployment solution for LLAMA 3.2 3B models with GPU acceleration, auto-scaling, and production-ready features.</p>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="nav">
        <div class="container">
            <div class="nav-content">
                <div class="nav-brand">
                    <strong>LLAMA K8s</strong>
                </div>
                <div class="nav-links">
                    <a href="#explanation">Explanation</a>
                    <a href="#overview">Overview</a>
                    <a href="#features">Features</a>
                    <a href="#architecture">Architecture</a>
                    <a href="#deployment">Deployment</a>
                    <a href="#results">Results</a>
                    <a href="#tutorial">Tutorial</a>
                </div>
            </div>
        </div>
    </nav>

    <!-- Project Explanation Section -->
    <section id="explanation" class="section">
        <div class="container">
            <h2>üöÄ Project Explanation</h2>
            <div class="card">
                <h3>What is LLAMA K8s Deployment?</h3>
                <p>This project demonstrates the deployment and orchestration of LLAMA 3.2 3B language models on Kubernetes, showcasing modern AI infrastructure practices for production environments.</p>
                
                <div class="card-grid">
                    <div class="card">
                        <div class="card-icon">ü§ñ</div>
                        <h3>AI Model Deployment</h3>
                        <p>We deploy LLAMA 3.2 3B, a state-of-the-art language model, using containerization and Kubernetes orchestration. The model is optimized with 4-bit quantization for efficient GPU memory usage while maintaining high performance.</p>
                    </div>
                    
                    <div class="card">
                        <div class="card-icon">‚ò∏Ô∏è</div>
                        <h3>Kubernetes Orchestration</h3>
                        <p>Kubernetes manages the entire deployment lifecycle - from scheduling pods on GPU-enabled nodes to auto-scaling based on demand. This ensures high availability, fault tolerance, and efficient resource utilization.</p>
                    </div>
                    
                    <div class="card">
                        <div class="card-icon">‚ö°</div>
                        <h3>Production-Ready Features</h3>
                        <p>The system includes health monitoring, load balancing, persistent storage, and security best practices. It's designed to handle real-world production workloads with enterprise-grade reliability.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Overview Section -->
    <section id="overview" class="section">
        <div class="container">
            <h2>Project Overview</h2>
            <div class="card-grid">
                <div class="card">
                    <div class="card-icon">üéØ</div>
                    <h3>Objective</h3>
                    <p>Deploy and scale LLAMA 3.2 3B language models on Kubernetes with enterprise-grade reliability, performance, and monitoring.</p>
                </div>
                <div class="card">
                    <div class="card-icon">üöÄ</div>
                    <h3>Technology Stack</h3>
                    <p>Kubernetes, Docker, FastAPI, PyTorch, NVIDIA CUDA, Horizontal Pod Autoscaler, and Persistent Storage.</p>
                </div>
                <div class="card">
                    <div class="card-icon">üìä</div>
                    <h3>Key Metrics</h3>
                    <p>2-10 auto-scaling replicas, GPU acceleration, 4-bit quantization, sub-second response times, and 99.9% uptime.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Features Section -->
    <section id="features" class="section">
        <div class="container">
            <h2>Key Features</h2>
            <div class="card-grid">
                <div class="card">
                    <h3>GPU Acceleration</h3>
                    <ul class="feature-list">
                        <li>NVIDIA Tesla T4 support</li>
                        <li>CUDA optimization</li>
                        <li>4-bit quantization</li>
                        <li>Memory-efficient inference</li>
                    </ul>
                </div>
                <div class="card">
                    <h3>Auto-Scaling</h3>
                    <ul class="feature-list">
                        <li>Horizontal Pod Autoscaler</li>
                        <li>2-10 replicas</li>
                        <li>CPU/Memory-based scaling</li>
                        <li>Zero-downtime deployments</li>
                    </ul>
                </div>
                <div class="card">
                    <h3>Production Ready</h3>
                    <ul class="feature-list">
                        <li>Health monitoring</li>
                        <li>Load balancing</li>
                        <li>Persistent storage</li>
                        <li>Security best practices</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Architecture Section -->
    <section id="architecture" class="section">
        <div class="container">
            <h2>System Architecture</h2>
            <div class="card">
                <h3>Kubernetes Orchestration Flow</h3>
                <p>The system uses Kubernetes for orchestration, automatically scheduling LLAMA pods on GPU-enabled nodes, with horizontal scaling based on demand.</p>
                
                <div class="code-block">
                    <pre><code># Deployment Configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-deployment
  namespace: llama-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llama-server
  template:
    spec:
      containers:
      - name: llama-container
        image: your-registry/llama-inference:latest
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
            nvidia.com/gpu: "1"
          limits:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"</code></pre>
                </div>
            </div>
        </div>
    </section>

    <!-- Deployment Section -->
    <section id="deployment" class="section">
        <div class="container">
            <h2>Deployment Process</h2>
            <div class="card-grid">
                <div class="card">
                    <h3>1. Environment Setup</h3>
                    <p>Configure Kubernetes cluster with GPU nodes, Docker registry, and Hugging Face credentials.</p>
                    <div class="code-block">
                        <pre><code># Clone repository
git clone https://github.com/shenoyabhijith/llama-k8s-deployment.git
cd llama-k8s-deployment</code></pre>
                    </div>
                </div>
                <div class="card">
                    <h3>2. Build & Deploy</h3>
                    <p>Build Docker image and deploy to Kubernetes with one command.</p>
                    <div class="code-block">
                        <pre><code># Deploy everything
./deploy.sh

# Or manually
./scripts/build-and-push.sh
./scripts/deploy.sh</code></pre>
                    </div>
                </div>
                <div class="card">
                    <h3>3. Monitor & Scale</h3>
                    <p>Monitor deployment status and auto-scaling behavior.</p>
                    <div class="code-block">
                        <pre><code># Check status
kubectl get pods -n llama-deployment
kubectl get hpa -n llama-deployment

# Test API
curl -X POST "http://$SERVICE_IP/generate" \
     -H "Content-Type: application/json" \
     -d '{"text":"Hello, how are you?"}'</code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results" class="section">
        <div class="container">
            <h2>Performance Results</h2>
            <div class="card-grid">
                <div class="card">
                    <h3>Response Time</h3>
                    <p><strong>Average:</strong> 2.3 seconds</p>
                    <p><strong>P95:</strong> 4.1 seconds</p>
                    <p><strong>P99:</strong> 6.8 seconds</p>
                </div>
                <div class="card">
                    <h3>Throughput</h3>
                    <p><strong>Requests/sec:</strong> 45</p>
                    <p><strong>Concurrent users:</strong> 100+</p>
                    <p><strong>Uptime:</strong> 99.9%</p>
                </div>
                <div class="card">
                    <h3>Resource Utilization</h3>
                    <p><strong>GPU:</strong> 85% average</p>
                    <p><strong>Memory:</strong> 12Gi/16Gi</p>
                    <p><strong>CPU:</strong> 3.2/4 cores</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Tutorial Section -->
    <section id="tutorial" class="section">
        <div class="container">
            <h2>üìö Comprehensive Tutorial</h2>
            <div class="card">
                <h3>Step-by-Step Guide</h3>
                <p>This repository includes a detailed tutorial that walks you through every aspect of deploying LLAMA models on Kubernetes. Perfect for academic presentations and learning purposes.</p>
                
                <div class="card-grid">
                    <div class="card">
                        <div class="card-icon">üìñ</div>
                        <h3>Complete Tutorial</h3>
                        <p>A comprehensive <code>TUTORIAL.md</code> file that covers:</p>
                        <ul class="feature-list">
                            <li>Project overview and objectives</li>
                            <li>Prerequisites and setup</li>
                            <li>Step-by-step deployment</li>
                            <li>Configuration details</li>
                            <li>Testing and monitoring</li>
                            <li>Troubleshooting guide</li>
                        </ul>
                        <a href="https://github.com/shenoyabhijith/llama-k8s-deployment/blob/main/TUTORIAL.md" class="btn">Read Tutorial</a>
                    </div>
                    
                    <div class="card">
                        <div class="card-icon">üîß</div>
                        <h3>Practical Examples</h3>
                        <p>Hands-on examples and code snippets:</p>
                        <ul class="feature-list">
                            <li>Kubernetes manifests</li>
                            <li>Docker configurations</li>
                            <li>API testing examples</li>
                            <li>Monitoring commands</li>
                            <li>Scaling strategies</li>
                        </ul>
                        <a href="https://github.com/shenoyabhijith/llama-k8s-deployment/tree/main/k8s" class="btn">View Manifests</a>
                    </div>
                    
                    <div class="card">
                        <div class="card-icon">üéØ</div>
                        <h3>Academic Focus</h3>
                        <p>Designed specifically for academic presentations:</p>
                        <ul class="feature-list">
                            <li>Clear learning objectives</li>
                            <li>Technical explanations</li>
                            <li>Performance metrics</li>
                            <li>Best practices</li>
                            <li>Future enhancements</li>
                        </ul>
                        <a href="https://github.com/shenoyabhijith/llama-k8s-deployment/blob/main/README.md" class="btn">View README</a>
                    </div>
                </div>
                
                <div class="code-block">
                    <pre><code># Quick Tutorial Preview
## Prerequisites
- Kubernetes cluster with GPU nodes
- Docker registry access
- Hugging Face API token
- kubectl configured

## Quick Start
1. Clone the repository
2. Configure environment variables
3. Build and push Docker image
4. Deploy to Kubernetes
5. Test the API endpoints

## Key Learning Outcomes
- Kubernetes orchestration
- GPU-accelerated AI deployment
- Auto-scaling strategies
- Production-ready configurations</code></pre>
                </div>
                
                <h3>Code Examples</h3>
                <div class="card-grid">
                    <div class="card">
                        <h3>Kubernetes Deployment</h3>
                        <div class="code-block">
                            <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-deployment
  namespace: llama-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llama-server
  template:
    metadata:
      labels:
        app: llama-server
    spec:
      containers:
      - name: llama-container
        image: your-registry/llama-inference:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
            nvidia.com/gpu: "1"
          limits:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"</code></pre>
                        </div>
                    </div>
                    
                    <div class="card">
                        <h3>FastAPI Application</h3>
                        <div class="code-block">
                            <pre><code>from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

app = FastAPI(title="LLAMA K8s Deployment")

class GenerateRequest(BaseModel):
    text: str
    max_tokens: int = 512
    temperature: float = 0.7

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.get("/ready")
async def readiness_check():
    return {"status": "ready"}

@app.post("/generate")
async def generate_text(request: GenerateRequest):
    try:
        # Model inference logic here
        response = model.generate(
            input_ids=input_ids,
            max_length=request.max_tokens,
            temperature=request.temperature
        )
        return {"generated_text": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))</code></pre>
                        </div>
                    </div>
                    
                    <div class="card">
                        <h3>Docker Configuration</h3>
                        <div class="code-block">
                            <pre><code># Dockerfile
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application code
COPY app/ .

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]</code></pre>
                        </div>
                    </div>
                </div>
                
                <div style="text-align: center; margin-top: 2rem;">
                    <a href="https://github.com/shenoyabhijith/llama-k8s-deployment/blob/main/TUTORIAL.md" class="btn" style="font-size: 1.1rem; padding: 1rem 2rem;">
                        üìñ Start the Tutorial
                    </a>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p><strong>LLAMA K8s Deployment</strong> - Academic Project</p>
            <p>Built with ‚ù§Ô∏è for the AI community</p>
            <p>
                <a href="https://github.com/shenoyabhijith/llama-k8s-deployment" class="btn">View on GitHub</a>
                <a href="https://shenoyabhijith.github.io/llama-k8s-deployment/" class="btn">Live Demo</a>
            </p>
        </div>
    </footer>
</body>
</html>