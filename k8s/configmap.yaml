# LLAMA Model Configuration
# This ConfigMap contains the model parameters and runtime configuration
# for the LLAMA 3.2 3B inference service.

apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-config
  namespace: llama-deployment
  labels:
    app: llama-server
    component: config
    version: "3.2"
data:
  # Model configuration
  MODEL_NAME: "meta-llama/Llama-3.2-3B"  # HuggingFace model identifier
  MODEL_REVISION: "main"                  # Model version/branch
  
  # Inference parameters
  MAX_TOKENS: "512"      # Maximum output length
  TEMPERATURE: "0.7"     # Sampling temperature (0.0-1.0)
  TOP_P: "0.95"         # Nucleus sampling parameter
  TOP_K: "50"           # Top-k sampling parameter
  
  # Performance tuning
  BATCH_SIZE: "8"       # Inference batch size
  NUM_THREADS: "4"      # Thread pool size
  TIMEOUT_SEC: "30"     # Request timeout
  
  # Memory optimization
  USE_4BIT: "true"              # Enable 4-bit quantization
  USE_NESTED_QUANT: "true"      # Enable nested quantization
  COMPUTE_DTYPE: "float16"      # Computation precision
  
  # Caching configuration
  CACHE_DIR: "/root/.cache/huggingface"
  MAX_CACHE_SIZE: "20Gi"
  
  # Monitoring
  ENABLE_METRICS: "true"
  METRICS_PORT: "8000"
  LOG_LEVEL: "INFO"